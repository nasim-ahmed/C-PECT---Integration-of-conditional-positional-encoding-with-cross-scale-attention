# C-PECT--Integration-of-conditional-positional-encoding-with-cross-scale-attention


In this approach, we propose C-PECT where we have explored how a transformer model performs when cross scale embeddings are integrated together. We first divide the input image into multiple patches. Every patch is reshaped by implementing a linear layer and then projected to form patches with embeddings. The dimensions of the embedded patches are reduced proportionately to the patch sizes of every stage. This reducing strategy enables us to generate multi scale feature maps at the same time keeping computational cost at linear. In our transformer layer we group the patches into multiple stages. In local grouping stage we group the neighboring patches together into blocks. In global grouping stage  the patches are assigned to same group after certain distance of the patches and blocks are formed. After this grouping mechanism self-attention are applied to the blocks. Our evaluation result using the MS COCO val2017 dataset yields an Accuracy Point (AP) of 40.0\% for 12 epochs and for 36 (3xCOCO) epochs  we achieved an AP of 43.7\% . After experimenting, our proposed network attains excellent results compared to state-of-the-art convolutional networks and on par with vision transformer variants including many concurrent works in computer vision tasks.
